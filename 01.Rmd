# Fabricating a Smarter Fab {#Machine-Learning-for-Semiconductors}

#### Keywords {-}

machine learning, semiconductors, fabrication, characterization, pattern recognition, efficiency, hardware, debugging

## Refocusing the Paradigm

When fabricating semiconductors, there are hundreds of process steps that might cause systematic electrical defects. These defects may cause chips to fail, resulting in lowered production yield [@turley_business_2003]. Therefore, electrical data is collected at every process step in which an engineer will encounter specific characteristics that define a certain electrical defect. As more of these characteristics are discovered, they begin to paint a picture of what the defect is; the same way that symptoms give doctors an idea of what is ailing the patient. The microelectronics industry is currently reliant on a handful of engineers to be able to see such symptoms in the electrical data and proceed to diagnose the electrical defect. However, this is a paradigm that is too focused in the present. The symptoms of past electrical defects are either forgotten or tied to certain engineers who may not be with the company forever. There is also no way to use current electrical data to anticipate whether a past defect is in the process of reemerging. With data science and machine learning, we can shift the current inefficient paradigm, into one where electrical defects leave behind a history of their symptoms that can help with future diagnoses. Simultaneously, such implementations would be practical in uncovering previously unfound correlations to the current defect.

## Harnessing Machine Learning

The first step is to harness machine learning. After an engineer has developed a good grasp of the symptoms classifying a defect, such as: cells failing as adjacent vertical pairs on a swath stretching vertically across the wafer, they can put these symptoms into a classifier to try and correctly identify that electrical defect. The classifier will analyze the electrical data of each wafer that gets processed and try to label them as having the defect or not. During the training period, the expert engineers will look at those wafers to correct the classifier of any mislabeling. This will help the classifier adjust its parameters and figure out the best way to identify this defect. The classifier might even find other correlating symptoms between these wafers that the engineers were not aware of. Any new information can be crucial when trying to fix the process flow. After engineers confirm that the system is sufficiently trained for an accuracy above 90%, the classifier can analyze older wafers to label any that the engineers may have missed. Engineers can data-mine this list of unhealthy wafers against a healthy sample to identify which step in the process flow is triggering these defects. Once this defect has been fixed, there will be a historical record of it, which would be valuable if such a defect reemerges. Like a healthcare database for diseases, engineers can input a list of symptoms into the system and then come back with possible defects. This will be extremely beneficial even if it doesn't diagnose the new defect exactly because it'll provide clues as to where to start looking.

<!-- ...or include images directly from the web. Cite your sources! -->
```{r machine-learning,echo=FALSE,fig.cap='[Steps 2 to 4 can be improved with this machine learning method](https://www.mckinsey.com/industries/semiconductors/our-insights/reimagining-fabs-advanced-analytics-in-semiconductor-manufacturing)',fig.align='center',out.width='50%'}
knitr::include_graphics('https://www.mckinsey.com/~/media/McKinsey/Industries/High%20Tech/Our%20Insights/Reimagining%20fabs%20Advanced%20analytics%20in%20semiconductor%20manufacturing/PNG_Reimaging_fabs_ex.ashx')
``` 
[@burkacky_reimagining_nodate]

## A More Efficient System

This system, however, comes with limitations. Completely revamping the current way to do things is not easy and definitely takes time and training. Furthermore, the idea is to continually teach the system new defects that appear and store them indefinitely. All the storage created from this new system, combined with the massive amounts of electrical data that is already being collected will take a lot of server space. Big companies like IBM with their own servers, may be able to support this implementation, but without proof of concept of how much money this can save, stakeholders may be hesitant about implementation. Engineers might worry that they are training a machine learning algorithm that may eventually replace them. This however, is not the case because as long as better and faster devices are developed, new and evolved electrical defects will always appear. Therefore, engineers are an irreplaceable part of this system.[@lucky_are_2016] If application of machine learning and big data is employed, the result is a more efficient fabrication paradigm. Engineers would be elevated to do their jobs better and faster all the while developing a catalog of all previous defects and their corresponding symptoms in the form of electrical data. 
